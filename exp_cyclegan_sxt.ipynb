{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import dnnlib\n",
    "import dnnlib.tflib as tflib\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "import imageio\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import config\n",
    "import gzip\n",
    "import json\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import warnings\n",
    "import cv2\n",
    "from keras.models import load_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KERAS_BACKEND']='tensorflow' # can choose theano, tensorflow, cntk\n",
    "os.environ['THEANO_FLAGS']='floatX=float32,device=cpu,optimizer=fast_run,dnn.library_path=/usr/lib'\n",
    "\n",
    "import keras.backend as K\n",
    "if os.environ['KERAS_BACKEND'] =='theano':\n",
    "    channel_axis=1\n",
    "    K.set_image_data_format('channels_first')\n",
    "    channel_first = True\n",
    "else:\n",
    "    K.set_image_data_format('channels_last')\n",
    "    channel_axis=-1\n",
    "    channel_first = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, ZeroPadding2D, BatchNormalization, Input, Dropout, DepthwiseConv2D\n",
    "from keras.layers import Conv2DTranspose, Reshape, Activation, Cropping2D, Flatten,PReLU\n",
    "from keras.layers import Concatenate,Multiply, Add, UpSampling2D,Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu\n",
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __conv_init(a):\n",
    "    print(\"conv_init\", a)\n",
    "    k = RandomNormal(0, 0.02)(a) # for convolution kernel\n",
    "    k.conv_weight = True    \n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic discriminator\n",
    "def conv2d(f, *a, **k):\n",
    "    return Conv2D(f, kernel_initializer = conv_init, *a, **k)\n",
    "def batchnorm():\n",
    "    return BatchNormalization(momentum=0.9, axis=channel_axis, epsilon=1.01e-5,\n",
    "                                   gamma_initializer = gamma_init)\n",
    "def BASIC_D(nc_in, ndf, max_layers=3, use_sigmoid=True):\n",
    "    \"\"\"DCGAN_D(nc, ndf, max_layers=3)\n",
    "       nc: channels\n",
    "       ndf: filters of the first layer\n",
    "       max_layers: max hidden layers\n",
    "    \"\"\"    \n",
    "    if channel_first:\n",
    "        input_a =  Input(shape=(nc_in, None, None))\n",
    "    else:\n",
    "        input_a = Input(shape=(None, None, nc_in))\n",
    "    _ = input_a\n",
    "    _ = conv2d(ndf, kernel_size=4, strides=2, padding=\"same\", name = 'First') (_)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    for layer in range(1, max_layers):        \n",
    "        out_feat = ndf * min(2**layer, 8)\n",
    "        _ = conv2d(out_feat, kernel_size=4, strides=2, padding=\"same\", \n",
    "                   use_bias=False, name = 'pyramid.{0}'.format(layer)             \n",
    "                        ) (_)\n",
    "        _ = batchnorm()(_, training=1)        \n",
    "        _ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    out_feat = ndf*min(2**max_layers, 8)\n",
    "    _ = ZeroPadding2D(1)(_)\n",
    "    _ = conv2d(out_feat, kernel_size=4,  use_bias=False, name = 'pyramid_last') (_)\n",
    "    _ = batchnorm()(_, training=1)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    # final layer\n",
    "    _ = ZeroPadding2D(1)(_)\n",
    "    _ = conv2d(1, kernel_size=4, name = 'final'.format(out_feat, 1), \n",
    "               activation = \"sigmoid\" if use_sigmoid else None) (_)    \n",
    "    return Model(inputs=[input_a], outputs=_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_init = RandomNormal(0, 0.02)\n",
    "gamma_init = RandomNormal(1., 0.02) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer, InputSpec\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class InstanceNormalization(Layer):\n",
    "    \"\"\"Instance normalization layer.\n",
    "    Normalize the activations of the previous layer at each step,\n",
    "    i.e. applies a transformation that maintains the mean activation\n",
    "    close to 0 and the activation standard deviation close to 1.\n",
    "    # Arguments\n",
    "        axis: Integer, the axis that should be normalized\n",
    "            (typically the features axis).\n",
    "            For instance, after a `Conv2D` layer with\n",
    "            `data_format=\"channels_first\"`,\n",
    "            set `axis=1` in `InstanceNormalization`.\n",
    "            Setting `axis=None` will normalize all values in each\n",
    "            instance of the batch.\n",
    "            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "            When the next layer is linear (also e.g. `nn.relu`),\n",
    "            this can be disabled since the scaling\n",
    "            will be done by the next layer.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a Sequential model.\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    # References\n",
    "        - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n",
    "        - [Instance Normalization: The Missing Ingredient for Fast Stylization](\n",
    "        https://arxiv.org/abs/1607.08022)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 axis=-1,\n",
    "                 epsilon=1e-8,\n",
    "                 center=True,\n",
    "                 scale=True,\n",
    "                 beta_initializer='zeros',\n",
    "                 gamma_initializer='ones',\n",
    "                 beta_regularizer=None,\n",
    "                 gamma_regularizer=None,\n",
    "                 beta_constraint=None,\n",
    "                 gamma_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(InstanceNormalization, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        ndim = len(input_shape)\n",
    "        if self.axis == 0:\n",
    "            raise ValueError('Axis cannot be zero')\n",
    "\n",
    "        if (self.axis is not None) and (ndim == 2):\n",
    "            raise ValueError('Cannot specify axis for rank 1 tensor')\n",
    "\n",
    "        self.input_spec = InputSpec(ndim=ndim)\n",
    "\n",
    "        if self.axis is None:\n",
    "            shape = (1,)\n",
    "        else:\n",
    "            shape = (input_shape[self.axis],)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(shape=shape,\n",
    "                                         name='gamma',\n",
    "                                         initializer=self.gamma_initializer,\n",
    "                                         regularizer=self.gamma_regularizer,\n",
    "                                         constraint=self.gamma_constraint)\n",
    "        else:\n",
    "            self.gamma = None\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(shape=shape,\n",
    "                                        name='beta',\n",
    "                                        initializer=self.beta_initializer,\n",
    "                                        regularizer=self.beta_regularizer,\n",
    "                                        constraint=self.beta_constraint)\n",
    "        else:\n",
    "            self.beta = None\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        reduction_axes = list(range(0, len(input_shape)))\n",
    "\n",
    "        if self.axis is not None:\n",
    "            del reduction_axes[self.axis]\n",
    "\n",
    "        del reduction_axes[0]\n",
    "\n",
    "        mean = K.mean(inputs, reduction_axes, keepdims=True)\n",
    "        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n",
    "        normed = (inputs - mean) / stddev\n",
    "\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        if self.axis is not None:\n",
    "            broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "        if self.scale:\n",
    "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "            normed = normed * broadcast_gamma\n",
    "        if self.center:\n",
    "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "            normed = normed + broadcast_beta\n",
    "        return normed\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'axis': self.axis,\n",
    "            'epsilon': self.epsilon,\n",
    "            'center': self.center,\n",
    "            'scale': self.scale,\n",
    "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
    "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
    "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
    "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
    "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
    "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
    "        }\n",
    "        base_config = super(InstanceNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize():\n",
    "#   return BatchNormalization(axis=get_filter_dim())\n",
    "    return InstanceNormalization()\n",
    "\n",
    "def scaleup(input, ngf, kss, strides, padding):\n",
    "#   x = Conv2DTranspose(ngf, kss, strides=strides, padding=padding)(input)\n",
    "\n",
    "    # upsample + conv\n",
    "    x = UpSampling2D(strides)(input)\n",
    "    x = Conv2D(ngf, kss, padding=padding)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def res_block(input, filters, kernel_size=(3,3), strides=(1,1)):\n",
    "    # conv_block:add(nn.SpatialReflectionPadding(1, 1, 1, 1))\n",
    "    # conv_block:add(nn.SpatialConvolution(dim, dim, 3, 3, 1, 1, p, p))\n",
    "    # conv_block:add(normalization(dim))\n",
    "    # conv_block:add(nn.ReLU(true))\n",
    "    \n",
    "    x = input\n",
    "    x = Lambda(lambda x: tf.pad(x, [[0,0], [1,1], [1,1], [0,0]], 'REFLECT'))(x)\n",
    "    x = Conv2D(filters=filters, \n",
    "              kernel_size=kernel_size,\n",
    "              strides=strides)(x)\n",
    "    x = normalize()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Lambda(lambda x: tf.pad(x, [[0,0], [1,1], [1,1], [0,0]], 'REFLECT'))(x)\n",
    "    x = Conv2D(filters=filters, \n",
    "              kernel_size=kernel_size,\n",
    "              strides=strides)(x)\n",
    "    x = normalize()(x)\n",
    "\n",
    "#   merged = Concatenate(axis=get_filter_dim())([input, x])\n",
    "    merged = Add()([input, x])\n",
    "    return merged\n",
    "\n",
    "def resnet_6blocks(input_shape, output_nc, ngf, **kwargs):\n",
    "    ks = 3\n",
    "    f = 7\n",
    "    p = int((f-1)/2)\n",
    "\n",
    "    input = Input(input_shape)\n",
    "    # local e1 = data - nn.SpatialReflectionPadding(p, p, p, p) - nn.SpatialConvolution(3, ngf, f, f, 1, 1) - normalization(ngf) - nn.ReLU(true)\n",
    "    x = input\n",
    "    x = Lambda(lambda x: tf.pad(x, [[0,0], [3,3], [3,3], [0,0]], 'REFLECT'))(x)\n",
    "    x = Conv2D(ngf, (f,f))(x)\n",
    "    x = normalize()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # local e2 = e1 - nn.SpatialConvolution(ngf, ngf*2, ks, ks, 2, 2, 1, 1) - normalization(ngf*2) - nn.ReLU(true)\n",
    "    x = Conv2D(ngf*2, (ks,ks), strides=(2,2), padding='same')(x)\n",
    "    x = normalize()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # local e3 = e2 - nn.SpatialConvolution(ngf*2, ngf*4, ks, ks, 2, 2, 1, 1) - normalization(ngf*4) - nn.ReLU(true)\n",
    "    x = Conv2D(ngf*4, (ks,ks), strides=(2,2), padding='same')(x)\n",
    "    x = normalize()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # local d1 = e3 - build_res_block(ngf*4, padding_type) - build_res_block(ngf*4, padding_type) - build_res_block(ngf*4, padding_type) \n",
    "    #  - build_res_block(ngf*4, padding_type) - build_res_block(ngf*4, padding_type) - build_res_block(ngf*4, padding_type)\n",
    "    x = res_block(x, ngf*4)\n",
    "    x = res_block(x, ngf*4)\n",
    "    x = res_block(x, ngf*4)\n",
    "    x = res_block(x, ngf*4)\n",
    "    x = res_block(x, ngf*4)\n",
    "    x = res_block(x, ngf*4)\n",
    "\n",
    "    # local d2 = d1 - nn.SpatialFullConvolution(ngf*4, ngf*2, ks, ks, 2, 2, 1, 1,1,1) - normalization(ngf*2) - nn.ReLU(true)\n",
    "    # x = Conv2DTranspose(ngf*2, (ks,ks), strides=(2,2), padding='same')(x)\n",
    "    x = scaleup(x, ngf*2, (ks, ks), strides=(2,2), padding='same')\n",
    "    x = normalize()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # local d3 = d2 - nn.SpatialFullConvolution(ngf*2, ngf, ks, ks, 2, 2, 1, 1,1,1) - normalization(ngf) - nn.ReLU(true)\n",
    "    # x = Conv2DTranspose(ngf, (ks,ks), strides=(2,2), padding='same')(x)\n",
    "    x = scaleup(x, ngf, (ks, ks), strides=(2,2), padding='same')\n",
    "    x = normalize()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # local d4 = d3 - nn.SpatialReflectionPadding(p, p, p, p) - nn.SpatialConvolution(ngf, output_nc, f, f, 1, 1) - nn.Tanh()\n",
    "    x = Lambda(lambda x: tf.pad(x, [[0,0], [3,3], [3,3], [0,0]], 'REFLECT'))(x)\n",
    "    x = Conv2D(output_nc, (f,f))(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    \n",
    "    model = Model(input, [x], name=kwargs.get('name',None))\n",
    "    print('Model resnet 6blocks:')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def res_block(input, filters, kernel_size=(3,3), strides=(1,1)):\n",
    "    \n",
    "    x = input\n",
    "    x = Lambda(lambda x: tf.pad(x, [[0,0], [1,1], [1,1], [0,0]], 'REFLECT'))(x)\n",
    "    x = Conv2D(filters=filters, \n",
    "              kernel_size=kernel_size,\n",
    "              strides=strides)(x)\n",
    "    x = normalize()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Lambda(lambda x: tf.pad(x, [[0,0], [1,1], [1,1], [0,0]], 'REFLECT'))(x)\n",
    "    x = Conv2D(filters=filters, \n",
    "              kernel_size=kernel_size,\n",
    "              strides=strides)(x)\n",
    "    x = normalize()(x)\n",
    "\n",
    "    merged = Add()([input, x])\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_sxt(input_shape=(512,512,4), output_nc=4, ngf=64):\n",
    "    \n",
    "    ks = 3\n",
    "    f = 7\n",
    "    p = int((f-1)/2)\n",
    "    \n",
    "    list_info_s =[]\n",
    "    list_info_t =[]\n",
    "\n",
    "    input_s = Input(input_shape)\n",
    "    input_t = Input(input_shape)\n",
    "    \n",
    "    s_layer = input_s\n",
    "    \n",
    "    s_layer = Conv2D(ngf, (f,f))(s_layer)\n",
    "    s_layer = normalize()(s_layer)\n",
    "    s_layer = Activation('relu')(s_layer)\n",
    "    \n",
    "    s_layer = Conv2D(ngf*2, (3,3), strides=(2,2), padding='same')(s_layer)\n",
    "    s_layer = normalize()(s_layer)\n",
    "    s_layer = Activation('relu')(s_layer)\n",
    "    \n",
    "    list_info_s.append(s_layer)\n",
    "\n",
    "    s_layer = Conv2D(ngf*4, (3,3), strides=(2,2), padding='same')(s_layer)\n",
    "    s_layer = normalize()(s_layer)\n",
    "    s_layer = Activation('relu')(s_layer)\n",
    "    \n",
    "    list_info_s.append(s_layer)\n",
    "    \n",
    "    \n",
    "    t_layer = input_t\n",
    "    \n",
    "    t_layer = Conv2D(ngf, (f,f))(t_layer)\n",
    "    t_layer = normalize()(t_layer)\n",
    "    t_layer = Activation('relu')(t_layer)\n",
    "    \n",
    "    t_layer = Conv2D(ngf*2, (3,3), strides=(2,2), padding='same')(t_layer)\n",
    "    t_layer = normalize()(t_layer)\n",
    "    t_layer = Activation('relu')(t_layer)\n",
    "    \n",
    "    list_info_t.append(t_layer)\n",
    "\n",
    "    t_layer = Conv2D(ngf*4, (3,3), strides=(2,2), padding='same')(t_layer)\n",
    "    t_layer = normalize()(t_layer)\n",
    "    t_layer = Activation('relu')(t_layer)\n",
    "    \n",
    "    list_info_t.append(t_layer)\n",
    "    \n",
    "    s_layer = Conv2D(ngf*2, (3,3), strides=(1,1), padding='same')(s_layer)\n",
    "    t_layer = Conv2D(ngf*2, (3,3), strides=(1,1), padding='same')(t_layer)\n",
    "    r_s_layer = r_t_layer = Multiply()([s_layer,t_layer])\n",
    "    \n",
    "    s_layer = Concatenate()([s_layer,r_s_layer])\n",
    "    t_layer = Concatenate()([t_layer,r_t_layer])\n",
    "    \n",
    "    s_layer = res_block(s_layer, ngf*4)\n",
    "    t_layer = res_block(t_layer, ngf*4)\n",
    "    \n",
    "    s_layer = t_layer = Multiply()([s_layer,t_layer])\n",
    "    \n",
    "    s_layer = res_block(s_layer, ngf*4)\n",
    "    t_layer = res_block(t_layer, ngf*4)\n",
    "    \n",
    "    s_layer = t_layer = Multiply()([s_layer,t_layer])\n",
    "    \n",
    "    s_layer = res_block(s_layer, ngf*4)\n",
    "    t_layer = res_block(t_layer, ngf*4)\n",
    "    \n",
    "    \n",
    "    s_layer = Concatenate()([s_layer,list_info_s[1]])\n",
    "    t_layer = Concatenate()([t_layer,list_info_t[1]])\n",
    "    \n",
    "    s_layer = Conv2DTranspose(ngf*2, (ks,ks), strides=(2,2), padding='same')(s_layer)\n",
    "    s_layer = normalize()(s_layer)\n",
    "    s_layer = Activation('relu')(s_layer)\n",
    "    \n",
    "    t_layer = Conv2DTranspose(ngf*2, (ks,ks), strides=(2,2), padding='same')(t_layer)\n",
    "    t_layer = normalize()(t_layer)\n",
    "    t_layer = Activation('relu')(t_layer)\n",
    "    \n",
    "    s_layer_0 = Lambda(lambda x: tf.pad(x, [[0,0], [1,0], [1,0], [0,0]], 'REFLECT'))(list_info_s[0])\n",
    "    t_layer_0 = Lambda(lambda x: tf.pad(x, [[0,0], [1,0], [1,0], [0,0]], 'REFLECT'))(list_info_t[0])\n",
    "    \n",
    "    s_layer = Concatenate()([s_layer,s_layer_0])\n",
    "    t_layer = Concatenate()([t_layer,t_layer_0])\n",
    "    \n",
    "    c_layer = Concatenate()([t_layer,s_layer])\n",
    "    \n",
    "    c_layer = Lambda(lambda x: tf.pad(x, [[0,0], [2,0], [2,0], [0,0]], 'REFLECT'))(c_layer)\n",
    "    c_layer = Conv2DTranspose(ngf, (ks,ks), strides=(2,2), padding='same')(c_layer)\n",
    "    c_layer = normalize()(c_layer)\n",
    "    c_layer = Activation('relu')(c_layer)\n",
    "    \n",
    "    \n",
    "    c_layer = Lambda(lambda x: tf.pad(x, [[0,0], [3,3], [3,3], [0,0]], 'REFLECT'))(c_layer)\n",
    "    c_layer = Conv2D(output_nc, (f,f))(c_layer)\n",
    "    c_layer = Activation('tanh')(c_layer)\n",
    "\n",
    "    \n",
    "    model = Model([input_s,input_t], c_layer)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_51 (InputLayer)           (None, 512, 512, 4)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_52 (InputLayer)           (None, 512, 512, 4)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_410 (Conv2D)             (None, 506, 506, 64) 12608       input_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_413 (Conv2D)             (None, 506, 506, 64) 12608       input_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_403 (Ins (None, 506, 506, 64) 128         conv2d_410[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_406 (Ins (None, 506, 506, 64) 128         conv2d_413[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_315 (Activation)     (None, 506, 506, 64) 0           instance_normalization_403[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_318 (Activation)     (None, 506, 506, 64) 0           instance_normalization_406[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_411 (Conv2D)             (None, 253, 253, 128 73856       activation_315[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_414 (Conv2D)             (None, 253, 253, 128 73856       activation_318[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_404 (Ins (None, 253, 253, 128 256         conv2d_411[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_407 (Ins (None, 253, 253, 128 256         conv2d_414[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_316 (Activation)     (None, 253, 253, 128 0           instance_normalization_404[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_319 (Activation)     (None, 253, 253, 128 0           instance_normalization_407[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_412 (Conv2D)             (None, 127, 127, 256 295168      activation_316[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_415 (Conv2D)             (None, 127, 127, 256 295168      activation_319[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_405 (Ins (None, 127, 127, 256 512         conv2d_412[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_408 (Ins (None, 127, 127, 256 512         conv2d_415[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_317 (Activation)     (None, 127, 127, 256 0           instance_normalization_405[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_320 (Activation)     (None, 127, 127, 256 0           instance_normalization_408[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_416 (Conv2D)             (None, 127, 127, 128 295040      activation_317[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_417 (Conv2D)             (None, 127, 127, 128 295040      activation_320[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "multiply_54 (Multiply)          (None, 127, 127, 128 0           conv2d_416[0][0]                 \n",
      "                                                                 conv2d_417[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_94 (Concatenate)    (None, 127, 127, 256 0           conv2d_416[0][0]                 \n",
      "                                                                 multiply_54[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_95 (Concatenate)    (None, 127, 127, 256 0           conv2d_417[0][0]                 \n",
      "                                                                 multiply_54[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_261 (Lambda)             (None, 129, 129, 256 0           concatenate_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_263 (Lambda)             (None, 129, 129, 256 0           concatenate_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_418 (Conv2D)             (None, 127, 127, 256 590080      lambda_261[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_420 (Conv2D)             (None, 127, 127, 256 590080      lambda_263[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_409 (Ins (None, 127, 127, 256 512         conv2d_418[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_411 (Ins (None, 127, 127, 256 512         conv2d_420[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_321 (Activation)     (None, 127, 127, 256 0           instance_normalization_409[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_322 (Activation)     (None, 127, 127, 256 0           instance_normalization_411[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda_262 (Lambda)             (None, 129, 129, 256 0           activation_321[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_264 (Lambda)             (None, 129, 129, 256 0           activation_322[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_419 (Conv2D)             (None, 127, 127, 256 590080      lambda_262[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_421 (Conv2D)             (None, 127, 127, 256 590080      lambda_264[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_410 (Ins (None, 127, 127, 256 512         conv2d_419[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_412 (Ins (None, 127, 127, 256 512         conv2d_421[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_107 (Add)                   (None, 127, 127, 256 0           concatenate_94[0][0]             \n",
      "                                                                 instance_normalization_410[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "add_108 (Add)                   (None, 127, 127, 256 0           concatenate_95[0][0]             \n",
      "                                                                 instance_normalization_412[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "multiply_55 (Multiply)          (None, 127, 127, 256 0           add_107[0][0]                    \n",
      "                                                                 add_108[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_265 (Lambda)             (None, 129, 129, 256 0           multiply_55[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_267 (Lambda)             (None, 129, 129, 256 0           multiply_55[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_422 (Conv2D)             (None, 127, 127, 256 590080      lambda_265[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_424 (Conv2D)             (None, 127, 127, 256 590080      lambda_267[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_413 (Ins (None, 127, 127, 256 512         conv2d_422[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_415 (Ins (None, 127, 127, 256 512         conv2d_424[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_323 (Activation)     (None, 127, 127, 256 0           instance_normalization_413[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_324 (Activation)     (None, 127, 127, 256 0           instance_normalization_415[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda_266 (Lambda)             (None, 129, 129, 256 0           activation_323[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_268 (Lambda)             (None, 129, 129, 256 0           activation_324[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_423 (Conv2D)             (None, 127, 127, 256 590080      lambda_266[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_425 (Conv2D)             (None, 127, 127, 256 590080      lambda_268[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_414 (Ins (None, 127, 127, 256 512         conv2d_423[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_416 (Ins (None, 127, 127, 256 512         conv2d_425[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_109 (Add)                   (None, 127, 127, 256 0           multiply_55[0][0]                \n",
      "                                                                 instance_normalization_414[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "add_110 (Add)                   (None, 127, 127, 256 0           multiply_55[0][0]                \n",
      "                                                                 instance_normalization_416[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "multiply_56 (Multiply)          (None, 127, 127, 256 0           add_109[0][0]                    \n",
      "                                                                 add_110[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_271 (Lambda)             (None, 129, 129, 256 0           multiply_56[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_269 (Lambda)             (None, 129, 129, 256 0           multiply_56[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_428 (Conv2D)             (None, 127, 127, 256 590080      lambda_271[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_426 (Conv2D)             (None, 127, 127, 256 590080      lambda_269[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_419 (Ins (None, 127, 127, 256 512         conv2d_428[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_417 (Ins (None, 127, 127, 256 512         conv2d_426[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_326 (Activation)     (None, 127, 127, 256 0           instance_normalization_419[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_325 (Activation)     (None, 127, 127, 256 0           instance_normalization_417[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda_272 (Lambda)             (None, 129, 129, 256 0           activation_326[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_270 (Lambda)             (None, 129, 129, 256 0           activation_325[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_429 (Conv2D)             (None, 127, 127, 256 590080      lambda_272[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_427 (Conv2D)             (None, 127, 127, 256 590080      lambda_270[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_420 (Ins (None, 127, 127, 256 512         conv2d_429[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_418 (Ins (None, 127, 127, 256 512         conv2d_427[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_112 (Add)                   (None, 127, 127, 256 0           multiply_56[0][0]                \n",
      "                                                                 instance_normalization_420[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "add_111 (Add)                   (None, 127, 127, 256 0           multiply_56[0][0]                \n",
      "                                                                 instance_normalization_418[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_97 (Concatenate)    (None, 127, 127, 512 0           add_112[0][0]                    \n",
      "                                                                 activation_320[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_96 (Concatenate)    (None, 127, 127, 512 0           add_111[0][0]                    \n",
      "                                                                 activation_317[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_48 (Conv2DTran (None, 254, 254, 128 589952      concatenate_97[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_47 (Conv2DTran (None, 254, 254, 128 589952      concatenate_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_422 (Ins (None, 254, 254, 128 256         conv2d_transpose_48[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_421 (Ins (None, 254, 254, 128 256         conv2d_transpose_47[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_328 (Activation)     (None, 254, 254, 128 0           instance_normalization_422[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda_274 (Lambda)             (None, 254, 254, 128 0           activation_319[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_327 (Activation)     (None, 254, 254, 128 0           instance_normalization_421[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda_273 (Lambda)             (None, 254, 254, 128 0           activation_316[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_99 (Concatenate)    (None, 254, 254, 256 0           activation_328[0][0]             \n",
      "                                                                 lambda_274[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_98 (Concatenate)    (None, 254, 254, 256 0           activation_327[0][0]             \n",
      "                                                                 lambda_273[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_100 (Concatenate)   (None, 254, 254, 512 0           concatenate_99[0][0]             \n",
      "                                                                 concatenate_98[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_275 (Lambda)             (None, 256, 256, 512 0           concatenate_100[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_49 (Conv2DTran (None, 512, 512, 64) 294976      lambda_275[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_423 (Ins (None, 512, 512, 64) 128         conv2d_transpose_49[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_329 (Activation)     (None, 512, 512, 64) 0           instance_normalization_423[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda_276 (Lambda)             (None, 518, 518, 64) 0           activation_329[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_430 (Conv2D)             (None, 512, 512, 4)  12548       lambda_276[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_330 (Activation)     (None, 512, 512, 4)  0           conv2d_430[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 9,930,308\n",
      "Trainable params: 9,930,308\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m=generator_sxt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netDA= gram_net_discriminator()\n",
    "netDB= gram_net_discriminator()\n",
    "netDA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngf=64\n",
    "netGB=resnet_6blocks((512,512,3),3,ngf)\n",
    "netGA=resnet_6blocks((512,512,3),3,ngf)\n",
    "netGA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "if use_lsgan:\n",
    "    loss_fn = lambda output, target : K.mean(K.abs(K.square(output-target)))\n",
    "else:\n",
    "    loss_fn = lambda output, target : -K.mean(K.log(output+1e-12)*target+K.log(1-output+1e-12)*(1-target))\n",
    "\n",
    "\n",
    "def cycle_variables(netG1, netG2):\n",
    "    real_input_s = netG1.inputs[0]\n",
    "    fake_output_s = netG1.outputs[0]\n",
    "    \n",
    "    real_input_t = netG1.inputs[1]\n",
    "    \n",
    "    rec_input = netG2([fake_output_s,real_input_s])\n",
    "    \n",
    "    fn_generate = K.function([real_input_s,real_input_t], [fake_output_s, rec_input])\n",
    "    return real_input_s,real_input_t,fake_output_s, rec_input, fn_generate\n",
    "\n",
    "real_A_s, real_A_t, fake_B, rec_A, cycleA_generate  = cycle_variables(netGB, netGA)\n",
    "real_B_s, real_B_t, fake_A, rec_B, cycleB_generate = cycle_variables(netGA, netGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptual_loss00(y_true, y_pred): \n",
    "    n_v = tf.constant([[[[103.939, 116.779, 123.68]]]])\n",
    "    y_true = ((y_true[...,::2,::2,::-1]+1)*127.5 - n_v)\n",
    "    y_pred = ((y_pred[...,::2,::2,::-1]+1)*127.5 - n_v)\n",
    "\n",
    "    \n",
    "    true_loss = loss_model(y_true)\n",
    "    pred_loss = loss_model(y_pred)\n",
    "    loss = 0\n",
    "    loss +=1*K.mean(K.abs(y_pred - y_true))\n",
    "    loss += 0.001*K.mean(K.square(true_loss[0] - pred_loss[0]))\n",
    "    loss += 0.0005*K.mean(K.square(true_loss[1] - pred_loss[1]))\n",
    "    return loss\n",
    "\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "vgg = VGG16(include_top=False, weights='imagenet', input_shape=(256, 256, 3)) \n",
    "loss_model = Model(inputs=vgg.input, outputs=[vgg.get_layer('block1_conv2').output,\n",
    "                                              vgg.get_layer('block2_conv2').output]) \n",
    "loss_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_loss(netD, real_s,real_t, fake, rec):\n",
    "    output_real = netD([real_s])\n",
    "    output_fake = netD([fake])\n",
    "    loss_D_real = loss_fn(output_real, K.ones_like(output_real))\n",
    "    loss_D_fake = loss_fn(output_fake, K.zeros_like(output_fake))\n",
    "    loss_G = loss_fn(output_fake, K.ones_like(output_fake))\n",
    "    loss_D = loss_D_real+loss_D_fake\n",
    "    loss_cyc = K.mean(K.abs(rec-real))+1.*perceptual_loss00(real,rec)\n",
    "\n",
    "    return loss_D, loss_G, loss_cyc\n",
    "\n",
    "loss_DA, loss_GA, loss_cycA = D_loss(netDA, real_A_s, real_A_t, fake_B, rec_A)\n",
    "loss_DB, loss_GB, loss_cycB = D_loss(netDB, real_B_s, real_B_t, fake_A, rec_B)\n",
    "loss_cyc = loss_cycA+loss_cycB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_G = loss_GA+loss_GB+Î»*loss_cyc\n",
    "loss_D = loss_DA+loss_DB\n",
    "\n",
    "weightsD = netDA.trainable_weights + netDB.trainable_weights\n",
    "weightsG = netGA.trainable_weights + netGB.trainable_weights\n",
    "\n",
    "training_updates = Adam(lr=lrD*0.1, beta_1=0.5).get_updates(weightsD,[],loss_D)\n",
    "netD_train = K.function([real_A, real_B],[loss_DA/2, loss_DB/2], training_updates)\n",
    "training_updates = Adam(lr=lrG*0.1, beta_1=0.5).get_updates(weightsG,[], loss_G)\n",
    "netG_train = K.function([real_A, real_B], [loss_GA, loss_GB, loss_cyc], training_updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(s_data,t_data, batchsize):\n",
    "    length = len(data)\n",
    "    epoch = i = 0\n",
    "    tmpsize = None    \n",
    "    while True:\n",
    "        size = tmpsize if tmpsize else batchsize\n",
    "        if i+size > length:\n",
    "            shuffle(data)\n",
    "            i = 0\n",
    "            epoch+=1   \n",
    "        idx = np.random.randint(0,data.shape[0], size)\n",
    "        rtn0 = s_data[idx] \n",
    "        rtn1 = t_data[idx] \n",
    "        i+=size\n",
    "        tmpsize = yield epoch, np.float32([rtn0,rtn1])       \n",
    "\n",
    "def minibatchAB(dataA, dataB, batchsize):\n",
    "    batchA=minibatch(dataA,dataB, batchsize)\n",
    "    batchB=minibatch(dataB,dataA, batchsize)\n",
    "    tmpsize = None    \n",
    "    while True:        \n",
    "        ep1, A = batchA.send(tmpsize)\n",
    "        ep2, B = batchB.send(tmpsize)\n",
    "        tmpsize = yield max(ep1, ep2), A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = (30,30)\n",
    "def showX(X, rows=1):\n",
    "    assert X.shape[0]%rows == 0\n",
    "    int_X = ( (X+1)/2*255).clip(0,255).astype('uint8')\n",
    "    if channel_first:\n",
    "        int_X = np.moveaxis(int_X.reshape(-1,3,imageSize,imageSize), 1, 3)\n",
    "    else:\n",
    "        int_X = int_X.reshape(-1,imageSize,imageSize, 3)\n",
    "    int_X = int_X.reshape(rows, -1, imageSize, imageSize,3).swapaxes(1,2).reshape(rows*imageSize,-1, 3)\n",
    "    plt.imshow(int_X.astype('uint8'))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = minibatchAB(train_A, train_B, 6)\n",
    "\n",
    "_, A, B = next(train_batch)\n",
    "showX(A)\n",
    "showX(B)\n",
    "\n",
    "del train_batch, A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showG(A,B):\n",
    "    assert A.shape==B.shape\n",
    "    def G(fn_generate, X,Y):\n",
    "\n",
    "        r = np.array([fn_generate([X[i:i+1],Y[i:i+1]]) for i in range(X.shape[0])])\n",
    "        return r.swapaxes(0,1)[:,:,0]        \n",
    "    rA = G(cycleA_generate, A,B)\n",
    "    rB = G(cycleB_generate, B,A)\n",
    "    arr = np.concatenate([A,B,rA[0],rB[0],rA[1],rB[1]])\n",
    "    #new = np.concatenate([rA[0][0],rB[0][0]],axis=1)\n",
    "    #plt.imshow(( (new+1)/2*255).clip(0,255).astype('uint8'))\n",
    "    #plt.show()\n",
    "    showX(arr, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.memmap('x_data.npy', dtype='float32', mode='r', shape=(len(find_x_data),512,512,4))[:,:,:,:3]\n",
    "y_data = np.memmap('y_data.npy', dtype='float32', mode='r', shape=(len(find_x_data),512,512,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "t0 = time.time()\n",
    "\n",
    "std_loss_A = []\n",
    "mean_loss_A =[]\n",
    "std_loss_B = []\n",
    "mean_loss_B =[]\n",
    "errCyc_sum = errGA_sum = errGB_sum = errDA_sum = errDB_sum = m_loss_A_sum = a_loss_A_sum=m_loss_B_sum = a_loss_B_sum = 0\n",
    "\n",
    "\n",
    "display_iters = 25\n",
    "#val_batch = minibatch(valAB, 6, direction)\n",
    "\n",
    "niter = 1500\n",
    "gen_iterations = 0\n",
    "epoch = 1200\n",
    "batchSize = 4\n",
    "train_batch = minibatchAB(x_data, y_data, batchSize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculte_fid(real_data,fake_data,fid_batch=512, id0 = 0):\n",
    "  \n",
    "        \n",
    "    fid_image_real = []\n",
    "    fid_image_fake = []\n",
    "    select_real=np.random.randint(0,real_data.shape[0], fid_batch)\n",
    "    select_fake=np.random.randint(0,fake_data.shape[0], fid_batch)\n",
    "      \n",
    "    for range_latent_real in real_data[select_real]:\n",
    "        range_image_real = correct_warp_with_landmarks(cv2.resize(my_Gs.components.synthesis.run(range_latent_real[None,...].reshape(-1,18,512), randomize_noise=False, **synthesis_kwargs)[0],(256,256)))[None,...]/255*2-1 \n",
    "\n",
    "        if id0==0:\n",
    "            rA =  netGA.predict(range_image_real)[0]\n",
    "        else:\n",
    "            rA = netGB.predict(range_image_real)[0]\n",
    "\n",
    "      \n",
    "        fid_image_fake.append(((rA+1)/2*255).clip(0,255).astype('uint8'))\n",
    "        \n",
    "    for range_latent_fake in fake_data[select_fake]:\n",
    "        fid_image_real.append(correct_warp_with_landmarks(cv2.resize(my_Gs.components.synthesis.run(range_latent_fake[None,...].reshape(-1,18,512), randomize_noise=False, **synthesis_kwargs)[0],(256,256))))\n",
    "        \n",
    "    fid_image_fake = np.array(fid_image_fake).astype(np.float32)\n",
    "    fid_image_real = np.array(fid_image_real).astype(np.float32)\n",
    "\n",
    "    fake_images = scale_images(fid_image_fake,(512,512,3))\n",
    "    new_real = scale_images(fid_image_real,(512,512,3))\n",
    "    images1 = preprocess_input(new_real)\n",
    "    images2 = preprocess_input(fake_images)\n",
    "      \n",
    "\n",
    "    fid = calculate_fid_0(model_fid, images1, images2)\n",
    "    print(fid)\n",
    "    return fid\n",
    "\n",
    "def calculate_fid_0(model, images1, images2):\n",
    "    # calculate activations\n",
    "    act1 = model.predict(images1)\n",
    "    act2 = model.predict(images2)\n",
    "    # calculate mean and covariance statistics\n",
    "    mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
    "    # calculate sum squared difference between means\n",
    "    ssdiff = numpy.sum((mu1 - mu2)**2.0)\n",
    "    # calculate sqrt of product between cov\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    # check and correct imaginary numbers from sqrt\n",
    "    if iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "     # calculate score\n",
    "    fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "    \n",
    "def scale_images(images, new_shape):\n",
    "    images_list = list()\n",
    "    for image in images:\n",
    "        # resize with nearest neighbor interpolation\n",
    "        new_image = resize(image, new_shape, 0)\n",
    "        # store\n",
    "        images_list.append(new_image)\n",
    "    return asarray(images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percept_loss = []\n",
    "fid_A_loss = []\n",
    "fid_B_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errCyc_sum = errGA_sum = errGB_sum = errDA_sum = errDB_sum=m_loss_A_sum = a_loss_A_sum=m_loss_B_sum = a_loss_B_sum = 0\n",
    "        \n",
    "display_iters = 200\n",
    "#val_batch = minibatch(valAB, 6, direction)\n",
    "\n",
    "niter = 1500\n",
    "gen_iterations = 0\n",
    "epoch = 1200\n",
    "batchSize = 4\n",
    "train_batch = minibatchAB(train_A, train_B, batchSize)\n",
    "\n",
    "\n",
    "while epoch < niter: \n",
    "    epoch, A, B = next(train_batch)        \n",
    "    errDA, errDB  = netD_train([A, B])\n",
    "    errDA_sum +=errDA\n",
    "    errDB_sum +=errDB\n",
    "\n",
    "    # epoch, trainA, trainB = next(train_batch)\n",
    "    errGA, errGB, errCyc = netG_train([A, B])\n",
    "    \n",
    "    errGA_sum += errGA\n",
    "    errGB_sum += errGB\n",
    "    errCyc_sum += errCyc\n",
    "    \n",
    "    gen_iterations+=1\n",
    "\n",
    "    if gen_iterations%display_iters==0:\n",
    "        clear_output()\n",
    "        print('[%d/%d][%d] Loss_D: %f %f Loss_G: %f %f loss_cyc %f '\n",
    "        % (epoch, niter, gen_iterations, errDA_sum/display_iters, errDB_sum/display_iters,\n",
    "           errGA_sum/display_iters, errGB_sum/display_iters, \n",
    "           errCyc_sum/display_iters), time.time()-t0)\n",
    "        _, A, B = train_batch.send(3)\n",
    "        percept_loss.append(errCyc_sum/display_iters)\n",
    "        showG(A,B)        \n",
    "        errCyc_sum = errGA_sum = errGB_sum = errDA_sum = errDB_sum=m_loss_A_sum = a_loss_A_sum=m_loss_B_sum = a_loss_B_sum = 0\n",
    "        plt.plot(percept_loss[1:])\n",
    "        plt.show()\n",
    "\n",
    "    if gen_iterations%100==0:\n",
    "\n",
    "        test_image_s0 = cv2.resize(imageio.imread('test/before_AH10x_0221_011_IMG_20200410_150014.jpg'),(512,512))[None,...]/255*2-1\n",
    "        test_image_t0 = cv2.resize(imageio.imread('test/after_AH10x_0221_011_IMG_20200410_150014.jpg'),(512,512))[None,...]/255*2-1\n",
    "        test_image_s1 = cv2.resize(imageio.imread('test/before_MJ10x_0024_IMG_20191226_135929_23_0_003_013_007_BE6I8878_8.2x.jpg'),(512,512))[None,...]/255*2-1\n",
    "        test_image_t1 = cv2.resize(imageio.imread('test/after_MJ10x_0024_IMG_20191226_135929.jpg'),(512,512))[None,...]/255*2-1\n",
    "\n",
    "        rA0 = netGB.predict([test_image_s0,test_image_t0])\n",
    "        rB0 = netGA.predict([test_image_t0,test_image_s0])\n",
    "        rA1 = netGB.predict([test_image_s1,test_image_t1])\n",
    "        rB1 = netGA.predict([test_image_t1,test_image_s1])\n",
    "        \n",
    "        test0 = np.concatenate([((test_image_s0[0]+1)/2*255)clip(0,255).astype('uint8'),((test_image_t0[0]+1)/2*255)clip(0,255).astype('uint8')],axis=0)\n",
    "        test0 = np.concatenate([test0,cv2.resize(((rA0[0]+1)/2*255)clip(0,255).astype('uint8'),(1024,1024))],axis=1)\n",
    "        \n",
    "        test1 = np.concatenate([((test_image_t0[0]+1)/2*255)clip(0,255).astype('uint8'),((test_image_s0[0]+1)/2*255)clip(0,255).astype('uint8')],axis=0)\n",
    "        test1 = np.concatenate([test1,cv2.resize(((rB0[0]+1)/2*255)clip(0,255).astype('uint8'),(1024,1024))],axis=1)\n",
    "        \n",
    "        \n",
    "        test2 = np.concatenate([((test_image_s1[0]+1)/2*255)clip(0,255).astype('uint8'),((test_image_t1[0]+1)/2*255)clip(0,255).astype('uint8')],axis=0)\n",
    "        test2 = np.concatenate([test2,cv2.resize(((rA1[0]+1)/2*255)clip(0,255).astype('uint8'),(1024,1024))],axis=1)\n",
    "        \n",
    "        test3 = np.concatenate([((test_image_t1[0]+1)/2*255)clip(0,255).astype('uint8'),((test_image_s1[0]+1)/2*255)clip(0,255).astype('uint8')],axis=0)\n",
    "        test3 = np.concatenate([test3,cv2.resize(((rB1[0]+1)/2*255)clip(0,255).astype('uint8'),(1024,1024))],axis=1)\n",
    "        \n",
    "        plt.imshow(test0)\n",
    "        plt.show()\n",
    "        plt.imshow(test1)\n",
    "        plt.show()\n",
    "        plt.imshow(test2)\n",
    "        plt.show()\n",
    "        plt.imshow(test3)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
